{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 과거 딥러닝의 근본적 문제점\n",
        "\n",
        "* 과적합\n",
        "* 기울기 소멸\n",
        "* 성능 하락"
      ],
      "metadata": {
        "id": "d4BM45MFAQs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문제 발생 원인\n",
        "\n",
        "* 과적합 - 훈련 데이터를 과도하게 학습하여, 검증/실제 데이터에 대한 오차가 증가함.\n",
        "\n",
        "* 기울기 소멸 - 은닉층이 많아지다 보면, 오차에 대한 계산도 많아져 오차가 크게 감소, 결국은 이에 대한 기울기가 소멸(0에 수렴)하여 학습이 불가해짐.\n",
        "\n",
        "* 성능 하락 - 경사 하강법은 오차가 가장 작게 되는 지점을 찾는데, 이 과정에서 시간 낭비, 적합점 탐색 불가, 발산 등의 성능이 하락하는 문제들이 발생."
      ],
      "metadata": {
        "id": "64xguKP5DiBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "7l7TGYvtSTrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 과적합 문제 해결 방안\n",
        "\n",
        "* 과적합 - 드롭아웃"
      ],
      "metadata": {
        "id": "wFWcAwpGCaQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 과적합 문제 발생\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# 데이터 전처리\n",
        "x_train = x_train.reshape(-1, 784) / 255.0\n",
        "x_test = x_test.reshape(-1, 784) / 255.0\n",
        "\n",
        "# 과적합을 위해 훈련 데이터 증강(의도)\n",
        "x_train = np.concatenate((x_train, x_train[:1000]))\n",
        "y_train = np.concatenate((y_train, y_train[:1000]))\n",
        "\n",
        "# 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# 모델 컴파일 및 학습\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=128, epochs=20, validation_data=(x_test, y_test))"
      ],
      "metadata": {
        "id": "sPHWw2cUADvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이 코드를 추가\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# 모델 구성\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(784,)))\n",
        "model.add(Dropout(0.5))  # 드롭아웃 층 추가\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))  # 드롭아웃 층 추가\n",
        "model.add(Dense(10, activation='softmax'))"
      ],
      "metadata": {
        "id": "w4Sh2OYdKmuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "드롭아웃은 입력된 데이터에 대해 학습하는 과정 중 학습에 사용되는 일부 노드(뉴런)들을 학습에서 제외시켜 과적합을 막을 수 있는 방법이다. 과적합 문제 뿐 아니라 모델이 더욱 향상된 일반화 성능을 가질 수 있게 해 준다."
      ],
      "metadata": {
        "id": "uBMuE2B-K2PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 드롭아웃 구성 예제\n",
        "class DropoutModel(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DropoutModel, self).__init__()\n",
        "        self.layer1 = torch.nn.Linear(784, 1200)\n",
        "        self.dropout1 = torch.nn.Dropout(0.5) # 50%의 노드를 무작위로 선택하여 사용하지 않겠다는 의미\n",
        "\n",
        "        self.layer2 = torch.nn.Linear(1200, 1200)\n",
        "        self.dropout2 = torch.nn.Dropout(0.5) # \"\n",
        "\n",
        "        self.layer3 = torch.nn.Linear(1200, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu( self.layer1(x) )\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        x = F.relu( self.layer2(x) )\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "df_4_4OFLcBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TDUz-o7eSQWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 기울기 소멸 문제 해결 방안\n",
        "\n",
        "* 기울기 소멸 - ReLU"
      ],
      "metadata": {
        "id": "vDYNED2YJ_lC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 시그모이드 함수의 정의\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "GOoXk1umPxw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyToch\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.sigmoid(x)"
      ],
      "metadata": {
        "id": "c1F9zF4qT-e7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "은닉층이 많은 신경망에서, 역전파 과정 중 오차가 0에 수렴하여 가중치에 대한 업데이트가 이루어지지 않는 현상이다."
      ],
      "metadata": {
        "id": "SNWXoDJ1P4zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "주 원인은 활성화 함수로 시그모이드 함수가 사용되기 때문인데, 시그모이드 함수는 0~1 사이의 값을 출력하며, 만약 작은 값이 시그모이드 함수를 사용한 역전파 과정에서 계속 곱해지면 기울기가 점차 감소한다.\n",
        "\n",
        "그렇기에 시그모이드 함수 대신 렐루 함수를 활성화 함수로 사용하면 이 문제가 해결된다."
      ],
      "metadata": {
        "id": "CGC_ufGERrI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU 함수의 정의\n",
        "def relu(x):\n",
        "    return max(0, x)"
      ],
      "metadata": {
        "id": "bYkvpTkUR8N7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "F.relu(x)"
      ],
      "metadata": {
        "id": "2P1LgOyfUB0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jo7dheOPSUsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "C9XvVuzdSM1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 성능 하락 문제 해결 방안\n",
        "\n",
        "* 성능 하락 - 확률적 경사 하강법 / 미니 배치 경사 하강법"
      ],
      "metadata": {
        "id": "oyvG5ze_KAa3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 일반적인 경사 하강법 알고리즘\n",
        "\n",
        "def gradient_descent(X, y, learning_rate, num_iterations):\n",
        "    num_samples, num_features = X.shape\n",
        "    weights = np.zeros(num_features)\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # 예측 계산\n",
        "        y_pred = np.dot(X, weights) + bias\n",
        "\n",
        "        # 오차 계산\n",
        "        error = y_pred - y\n",
        "\n",
        "        # 가중치와 편향의 업데이트\n",
        "        weights -= (learning_rate / num_samples) * np.dot(X.T, error)\n",
        "        bias -= (learning_rate / num_samples) * np.sum(error)\n",
        "\n",
        "    return weights, bias"
      ],
      "metadata": {
        "id": "ssTNDb-cTHeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "경사 하강법은 강력한 알고리즘이지만 지역 최솟값 수렴, 계산 시간 급증, 발산 등의 성능이 하락하는 문제가 발생할 가능성이 있다. 이를 해결하기 위해 미니 배치 경사 하강법, 확률적 경사 하강법 등을 사용한다."
      ],
      "metadata": {
        "id": "R41qdz3-TYWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 배치 경사 하강법\n",
        "배치 경사 하강법은 Batch, 말 그대로 경사 하강법에 대해 일괄처리를 한다는 뜻이다. 전체 데이터셋에 대한 오류를 계산한 뒤 기울기를 한 번만 계산하여 모델의 파라미터를 업데이트한다."
      ],
      "metadata": {
        "id": "QuRmCUBbUlJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 경사 하강법 알고리즘\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def batch_gradient_descent(X, y, learning_rate, num_iterations):\n",
        "    num_samples, num_features = X.shape\n",
        "    weights = np.zeros(num_features)\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # 예측 계산\n",
        "        y_pred = np.dot(X, weights) + bias\n",
        "\n",
        "        # 오차 계산\n",
        "        error = y_pred - y\n",
        "\n",
        "        # 가중치와 편향의 업데이트\n",
        "        weights -= (learning_rate / num_samples) * np.dot(X.T, error)\n",
        "        bias -= (learning_rate / num_samples) * np.sum(error)\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "XDR6m0D-UWAA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 미니 배치 경사 하강법\n",
        "배치 경사 하강법과 개념적으로는 비슷하지만, 일괄처리하는 묶음을 여러 개로 쪼갠다는 차이점이 있다. 즉, 전체 데이터셋을 미니 배치 여러 개로 나누고 각각의 미니 배치에 대한 일괄 처리 계산 후 그것들의 평균 기울기를 사용해 모델의 파라미터를 업데이트한다."
      ],
      "metadata": {
        "id": "RmE8aTcAVrtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 미니 배치 경사 하강법 알고리즘\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def mini_batch_gradient_descent(X, y, learning_rate, batch_size, num_iterations):\n",
        "    num_samples, num_features = X.shape\n",
        "    weights = np.zeros(num_features)\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # 배치 샘플 선택\n",
        "        indices = np.random.choice(num_samples, batch_size, replace=False)\n",
        "        X_batch = X[indices]\n",
        "        y_batch = y[indices]\n",
        "\n",
        "        # 예측 계산\n",
        "        y_pred = np.dot(X_batch, weights) + bias\n",
        "\n",
        "        # 오차 계산\n",
        "        error = y_pred - y_batch\n",
        "\n",
        "        # 가중치와 편향의 업데이트\n",
        "        weights -= (learning_rate / batch_size) * np.dot(X_batch.T, error)\n",
        "        bias -= (learning_rate / batch_size) * np.sum(error)\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "KK3n44LxWDu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 미니 배치 경사 하강법 구성 예제\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x_data = [ [1,2,3], [4,5,6], [7,8,9] ]\n",
        "        self.y_data = [ [12], [18], [11] ]\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.x_data)\n",
        "\n",
        "        def __getitem__(self, idx):\n",
        "            x = torch.FloatTensor(self.x_data[idx])\n",
        "            y = torch.FloatTensor(self.y_data[idx])\n",
        "            return x, y\n",
        "\n",
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader( dataset, batch_size=2, shuffle=True )"
      ],
      "metadata": {
        "id": "-2UHmKyoWPCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 확률적 경사 하강법\n",
        "확률적 경사 하강법은 임의로 선택한 데이터에 대해 기울기를 계산하고 가중치와 편향을 업데이트한다."
      ],
      "metadata": {
        "id": "-46wfi2FYVi3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 확률적 경사 하강법 알고리즘\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "def stochastic_gradient_descent(X, y, learning_rate, num_iterations):\n",
        "    num_samples, num_features = X.shape\n",
        "    weights = np.zeros(num_features)\n",
        "    bias = 0\n",
        "\n",
        "    for _ in range(num_iterations):\n",
        "        # 무작위로 훈련 샘플 선택\n",
        "        index = np.random.randint(num_samples)\n",
        "        x = X[index]\n",
        "        label = y[index]\n",
        "\n",
        "        # 예측 계산\n",
        "        y_pred = np.dot(x, weights) + bias\n",
        "\n",
        "        # 오차 계산\n",
        "        error = y_pred - label\n",
        "\n",
        "        # 가중치와 편향의 업데이트\n",
        "        weights -= learning_rate * error * x\n",
        "        bias -= learning_rate * error\n",
        "\n",
        "    return weights, bias\n"
      ],
      "metadata": {
        "id": "fqEVVJ9NcEjT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "확률적 경사 하강법은 무작위로 하나의 훈련 샘플을 선택해 연산을 진행하기 때문에 파라미터의 변경 폭이 불안정하나 속도가 비교적 빠르다."
      ],
      "metadata": {
        "id": "cETWRxExcVs3"
      }
    }
  ]
}