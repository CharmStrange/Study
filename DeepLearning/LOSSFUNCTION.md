# **손실 함수**
#### (*밑바닥부터 시작하는 딥러닝* 교재 참고)

딥러닝의 신경망에서 손실 함수는 신경망 처리 성능의 지표이다. 

신경망의 학습에선 최적의 **파라미터**(가중치와 편향), 즉 손실 함수의 값을 가장 작게 하는 파라미터를 찾는데, 여기서 미분 연산이 활발히 이루어진다. 

학습 과정에서 손실 함수의 값을 계속해서 미분하는데 이것은 손실 함수 값 변화와 CHAIN, 그러니까 미분 값에 따라 손실 함수의 값도 바뀌고, 바뀐 손실 함수 값에 따라 미분 값도 바뀌어 최적의 파라미터를 찾을 수 있게 된다.

---

## 1. 평균 제곱 오차(MSE)
결과(출력, 또는 예측) 값과 실제 값(레이블)의 차이를 오차로 두고, 이들을 제곱하여 모두 더한 후 데이터의 개수로 나누어 구한다.
```Python
import numpy as np

def MSE(n, y, Label):
    return (1/n) * np.sum( (y-Label)**2 )

# Label
L = np.array( [1, 0, 1, 1, 0] )

# Expected Output
ye = np.array( [0.8, 0.1, 1, 0.9, 0.3] )

print( MSE(5, ye, L) )
```
```Python
>>> 0.029999999999999995
```
수식에서 n이 아닌 2를 쓰기도 하는데 이는 미분했을 때 제곱의 2가 곱해지는 것을 상쇄하기 위해 존재한다.

## 2. 교차 엔트로피 오차(CEE)
